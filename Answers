1=C
2=D
3=C
4=A
5=B
6=B
7=C
8=B,C
9=A,C,D
10=A,B,D
11= OUTLIERS

An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal. Before abnormal observations can be singled out, it is necessary to characterize normal observations.

IQR
Using the Interquartile Rule to Find Outliers
Multiply the interquartile range (IQR) by 1.5 (a constant used to discern outliers). Add 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier. Subtract 1.5 x (IQR) from the first quartile.

12=  BAGGING AND BOOSTING
Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data. Boosting is an iterative technique which adjusts the weight of an observation based on the last classification.

13=  R2 SCORE
Adjusted R-squared value can be calculated based on value of r-squared, number of independent variables (predictors), total sample size. Every time you add a independent variable to a model, the R-squared increases, even if the independent variable is insignificant. It never declines.
    
    HOW IT IS CALCULATED?
    
    The R-squared formula is calculated by dividing the sum of the first errors by the sum of the second errors and subtracting the derivation from 1. Here’s what the r-squared equation looks like.
    
    R-Squared Formula and Equation
R-squared = 1 – (First Sum of Errors / Second Sum of Errors)

Keep in mind that this is the very last step in calculating the r-squared for a set of data point. There are several steps that you need to calculate before you can get to this point.

14=  NORMALISATION AND STANDARDISATION

     The terms normalization and standardization are sometimes used interchangeably, but they usually refer to different things. Normalization usually means to scale a variable to have a values between 0 and 1, while standardization transforms data to have a mean of zero and a standard deviation of 1.
     
     
  15=   CROSS-VALIDATION
  
      Cross-validation is a statistical method used to estimate the skill of machine learning models.

It is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.

In this tutorial, you will discover a gentle introduction to the k-fold cross-validation procedure for estimating the skill of machine learning models.


    ADVANTAGES
    
    1.Use All Your Data
    2. Get More Metrics
    3. Use Models Stacking
    4. Work with Dependent/Grouped Data
    5. Parameters Fine-Tuning
    
    DISADVANTAGES
    
    1. Process can become a lengthy.
    2.It depends on the number of observations.
    3. The method of cross validation is similar to the LoO CV.